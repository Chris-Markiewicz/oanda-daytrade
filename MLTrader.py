# MLTrader.py

import pandas as pd
import numpy as np
import tpqoa
from datetime import datetime, timedelta, timezone
import time
import pickle
import os
from sklearn.preprocessing import StandardScaler 

from BaseIndicator import BaseIndicator
from MovingAverages import MovingAverageFeature, MACrossover
from MomentumIndicators import LaggedReturnsFeature, RSIFeature, MACDFeature
from VolatilityIndicators import ATRFeature, BollingerBandsFeatures, VolatilitySTDFeature
from TimeFeatures import HourFeature, DayOfWeekFeature


class MLTrader(tpqoa.tpqoa):

    def __init__(self, conf_file, instrument, bar_length, lags, units, 
                 model_filename="trained_ml_model.pkl", 
                 scaler_filename="trained_scaler.pkl", # New parameter for scaler
                 fallback_model_template=None,
                 live_indicators_to_apply: list = None): # For live feature engineering
        super().__init__(conf_file)
        self.instrument = instrument
        self.bar_length = pd.to_timedelta(bar_length)
        self.tick_data = pd.DataFrame()
        self.raw_data = None 
        self.data = None # Stores resampled bars (e.g., 1-min bars with raw features)
        self.last_bar_time = None

        self.model = None
        self.scaler = None # New attribute for the scaler
        self.lags = lags # Used by LaggedReturnsFeature if applied live
        self.units = units
        self.position = 0
        
        self.model_filename = model_filename
        self.scaler_filename = scaler_filename # Store scaler filename
        self.fallback_model_template = fallback_model_template
        self.live_indicators_to_apply = live_indicators_to_apply if live_indicators_to_apply else []

        # This list should ideally be dynamically determined or match ModelTrainer.py
        # For now, we'll assume it's based on the indicators applied.
        # It will be populated after features are generated by self.apply_live_indicators()
        self.feature_names_for_model = [] 

        print(f"MLTrader initialized for {self.instrument} with {self.lags} base lags, trading {self.units} units.")
        self.load_model_and_scaler() # Modified loading function

        if self.model is None:
            print(f"CRITICAL: Model could not be loaded from {self.model_filename}.")
            # ... (fallback logic as before) ...
        if self.scaler is None and self.model is not None: # If model loaded but scaler didn't
            print(f"CRITICAL: Scaler could not be loaded from {self.scaler_filename}, but model was loaded. Predictions will be incorrect.")
            self.model = None # Invalidate model if scaler is missing, as predictions would be on unscaled data
            print("Model invalidated due to missing scaler.")


    def load_model_and_scaler(self):
        """Loads the ML model and the StandardScaler from files."""
        # Load Model
        if os.path.exists(self.model_filename):
            try:
                with open(self.model_filename, 'rb') as f:
                    self.model = pickle.load(f)
                print(f"Loaded pre-trained model from {self.model_filename}")
                if not hasattr(self.model, 'predict'):
                    print(f"Warning: Loaded object for model is not valid. Invalidating."); self.model = None
            except Exception as e:
                print(f"Error loading model from {self.model_filename}: {e}. Model not loaded."); self.model = None
        else:
            print(f"No pre-trained model file found at {self.model_filename}."); self.model = None

        # Load Scaler
        if os.path.exists(self.scaler_filename):
            try:
                with open(self.scaler_filename, 'rb') as f:
                    self.scaler = pickle.load(f)
                print(f"Loaded StandardScaler from {self.scaler_filename}")
                if not hasattr(self.scaler, 'transform'): # Basic check
                    print(f"Warning: Loaded object for scaler is not valid. Invalidating."); self.scaler = None
            except Exception as e:
                print(f"Error loading scaler from {self.scaler_filename}: {e}. Scaler not loaded."); self.scaler = None
        else:
            print(f"No StandardScaler file found at {self.scaler_filename}."); self.scaler = None

    def fetch_initial_data(self, days_for_features=10, granularity="S5"): # Increased days slightly
        # ... (Fetching logic similar to before, gets OHLCV into df_hist) ...
        if self.model is None and self.fallback_model_template is None:
            print("No model, skipping initial data fetch for ML."); return False
            
        print(f"Fetching initial data ({days_for_features} days of {granularity}) to generate latest features for {self.instrument}...")
        now_utc = datetime.now(timezone.utc).replace(tzinfo=None)
        end_api = now_utc - timedelta(microseconds=now_utc.microsecond)
        start_api = end_api - timedelta(days=days_for_features)

        try:
            df_hist = self.get_history(instrument=self.instrument,
                                       start=start_api, end=end_api,
                                       granularity=granularity, price="M", localize=False)
        except Exception as e:
            print(f"Error fetching initial OANDA data: {e}"); return False
        
        if df_hist.empty: print(f"No historical data fetched."); return False
        
        # Resample to self.bar_length, ensuring OHLCV columns
        ohlc_dict = {'o': 'first', 'h': 'max', 'l': 'min', 'c': 'last', 'volume': 'sum'}
        resampled_hist_data = df_hist.resample(self.bar_length, label="right").agg(ohlc_dict)
        resampled_hist_data.rename(columns={'c': 'price', 'o':'open', 'h':'high', 'l':'low'}, inplace=True)
        resampled_hist_data.dropna(subset=['price', 'open', 'high', 'low'], inplace=True)

        if resampled_hist_data.empty:
            print(f"Not enough hist data to form bars of length {self.bar_length}."); return False

        # Apply indicators to generate raw features
        df_with_raw_features, generated_feature_names = self.apply_live_indicators(resampled_hist_data)
        
        if df_with_raw_features is None or not generated_feature_names:
            print("Failed to generate features from historical data."); return False

        self.data = df_with_raw_features # Now self.data contains bars with raw features
        self.feature_names_for_model = generated_feature_names # Store the names of features model expects
        self.last_bar_time = self.data.index[-1]
        
        self.raw_data = df_hist # Store original S5 data for continuity with live ticks
        # Ensure raw_data has the instrument column name for on_success concatenation
        if self.instrument not in self.raw_data.columns and 'c' in self.raw_data.columns:
            self.raw_data.rename(columns={'c': self.instrument}, inplace=True)
        elif self.instrument not in self.raw_data.columns and 'price' in self.raw_data.columns: # If raw_hist_df was already price
             self.raw_data.rename(columns={'price': self.instrument}, inplace=True)


        print(f"Initial data & raw features fetched. Last hist bar: {self.last_bar_time}. Data shape: {self.data.shape}")
        return True

    def apply_live_indicators(self, df_ohlcv_input: pd.DataFrame):
        """
        Applies the configured list of indicators to the input DataFrame.
        This should mirror the feature engineering in ModelTrainer.py.
        Input DataFrame should have 'price', 'open', 'high', 'low', 'volume'.
        """
        if df_ohlcv_input.empty:
            return None, []
            
        df = df_ohlcv_input.copy()
        current_feature_names = []

        # Ensure 'returns' is calculated first if needed by any indicator
        if 'price' in df.columns:
            df['returns'] = np.log(df['price'] / df['price'].shift(1))
        else:
            print("Warning in apply_live_indicators: 'price' column missing for base returns calc.")
            # Some indicators might fail if they expect 'returns' and can't calc it.

        for indicator_instance in self.live_indicators_to_apply:
            # print(f"  Applying live: {indicator_instance.name}") # Debug
            try:
                df = indicator_instance.calculate(df) # Calculate modifies df in place or returns modified
                current_feature_names.extend(indicator_instance.get_feature_names())
            except Exception as e:
                print(f"    Error applying live indicator {indicator_instance.name}: {e}")
        
        current_feature_names = sorted(list(set(current_feature_names)))
        
        # Important: Drop NaNs created by indicators.
        # This ensures the last row has complete features for scaling and prediction.
        df.dropna(subset=current_feature_names, inplace=True) 
        
        # Filter out features that are all NaN or not generated
        valid_feature_names = [col for col in current_feature_names if col in df.columns and not df[col].isnull().all()]

        return df, valid_feature_names
    def on_success(self, time_stamp, bid, ask, **kwargs):
        print(self.ticks, end=" ", flush=True) 
        recent_tick_time = pd.to_datetime(time_stamp)
        mid_price = (bid + ask) / 2.0
        new_tick_df = pd.DataFrame({self.instrument: mid_price}, index=[recent_tick_time])
        
        if self.raw_data is None or self.raw_data.empty:
            self.raw_data = new_tick_df
        else:
            self.raw_data = pd.concat([self.raw_data, new_tick_df])
        
        if self.last_bar_time is None: # First run or after reset
            # Attempt to resample current raw_data to form initial bars
            # This requires OHLCV from raw stream, which simple mid_price doesn't provide.
            # For simplicity, we rely on fetch_initial_data to set self.data and self.last_bar_time.
            # If fetch_initial_data failed, this part might struggle unless raw_data is S5 OHLCV.
            # The current raw_data from stream is only mid-price.
            # Let's assume fetch_initial_data worked.
            if self.data is not None and not self.data.empty:
                 self.last_bar_time = self.data.index[-1]
            else:
                # Cannot form bars yet if fetch_initial_data didn't populate self.data
                return


        if recent_tick_time - self.last_bar_time >= self.bar_length:
            self.resample_and_trade()

    def resample_and_trade(self):
        if self.raw_data is None or self.raw_data.empty or len(self.raw_data) < 20: # Need some ticks
            return

        # Resample raw_data (which should be S5 OHLCV from history + S5 mid-price ticks from stream)
        # For live, we only get mid-price ticks. To form OHLC bars from stream, need more logic.
        # Current self.raw_data from stream is just self.instrument (mid-price).
        # To use indicators needing OHLCV, self.raw_data structure must be richer or indicators simpler.

        # Simple approach: use only the 'self.instrument' (mid-price) column from raw_data for live bar formation
        # This means live indicators can only use 'price' (derived from mid) and 'returns'.
        # This is a DISCREPANCY from training if training used OHLCV features.
        
        # --- MAJOR CONSIDERATION FOR LIVE FEATURE ENGINEERING ---
        # If your trained model uses features derived from Open, High, Low, Volume
        # (e.g., ATR, some TA-Lib functions), you CANNOT generate these accurately
        # in the live loop if self.raw_data only contains mid-price ticks.
        # You would need to either:
        # 1. Only use features derivable from 'price' (mid) and 'returns' for live.
        # 2. Fetch S5 OHLCV data periodically (e.g., every minute via get_history for the last minute)
        #    to construct proper OHLCV bars for live feature calculation. This adds latency.
        # For now, assume apply_live_indicators can work with what's available or that
        # fetch_initial_data provides a long enough history of OHLCV in self.data.
        
        # Let's resample the instrument column from raw_data.
        # If raw_data was primed with OHLCV, this needs to be handled.
        # For now, assuming live stream makes self.raw_data mostly a series of mid-prices.
        
        if self.instrument not in self.raw_data.columns:
            print(f"Instrument column '{self.instrument}' not in self.raw_data. Cannot resample.")
            return

        live_bars = self.raw_data[[self.instrument]].resample(self.bar_length, label='right').agg(
            {'open': 'first', 'high': 'max', 'low': 'min', self.instrument: 'last'}
        )
        live_bars.rename(columns={self.instrument: 'price'}, inplace=True) # Now we have OHLC and 'price' (close)

        if live_bars.empty or live_bars.index[-1] <= self.last_bar_time:
            return

        # Combine with historical data if any, then apply indicators
        # self.data should have been initialized by fetch_initial_data with features
        # We need to append new live_bars and recalculate features for the latest part.
        
        # Strategy: Keep self.data as the master list of bars with *raw* features.
        # Recalculate features on an extended window each time.
        
        # Append new raw bars (OHLC from stream if possible, or just 'price')
        # For simplicity, let's assume live_bars has the necessary OHLC columns for indicators
        # If not, then self.live_indicators_to_apply must only use 'price' and 'returns'.

        # Concatenate new live bars to existing self.data (which has features)
        # This is tricky because self.data already has features. We need to append raw bars, then re-feature.
        # Let's maintain a separate df for OHLCV bars.
        
        # Let's assume self.data from fetch_initial_data is THE GOLDEN SOURCE of OHLCV + features
        # And we only add new bars to it.

        # For live, it's often better to get latest full bar from API to ensure OHLC is good.
        # This stream-based OHLC creation from mid-prices is an approximation.

        # Simpler flow for live:
        # 1. Append new live_bar (with 'price', and if possible OHL) to a raw bar history.
        # 2. Take a window of this raw bar history.
        # 3. Apply all indicators to this window.
        # 4. Get the latest row of features.
        # 5. Scale.
        # 6. Predict.
        
        # If self.data was set by fetch_initial_data with features:
        # We need to append the new 'live_bars' (price and OHLC) and recompute features on tail
        # This is computationally intensive to do live.
        # A more common live approach: Calculate features on the latest bar using prior values.
        
        # Current Approach: self.data has bars with features.
        # We need to update self.data with new_bar_time and its features.
        
        # Let's assume self.data (from fetch_initial_data) is the source of truth for historical bars
        # And we are forming new bars from the stream.
        new_ohlc_bars_from_stream = self.raw_data.resample(self.bar_length, label='right').agg(
            open=(self.instrument, 'first'),
            high=(self.instrument, 'max'),
            low=(self.instrument, 'min'),
            price=(self.instrument, 'last'), # 'price' for close
            volume=(self.instrument, 'count') # 'volume' as tick count for the bar
        ).dropna()

        if new_ohlc_bars_from_stream.empty or new_ohlc_bars_from_stream.index[-1] <= self.last_bar_time:
            return

        # Get only the truly new bars
        newly_formed_bars_live = new_ohlc_bars_from_stream[new_ohlc_bars_from_stream.index > self.last_bar_time]
        if newly_formed_bars_live.empty:
            return
            
        # Append these new raw OHLCV bars to the historical self.data (which has features)
        # This requires self.data to be structured to accept new raw bars first.
        # Let's redefine self.data from fetch_initial_data to be just OHLCV, then apply features.
        
        # Revised flow:
        # self.ohlcv_data: stores all OHLCV bars (hist + live)
        # In resample_and_trade:
        # 1. Update self.ohlcv_data with new bars from stream.
        # 2. Apply indicators to a recent window of self.ohlcv_data to get df_with_latest_features.
        # 3. Scale latest features.
        # 4. Predict.

        # Assuming self.data is now just OHLCV (from fetch_initial_data or built up)
        # This part needs self.data to be just OHLCV initially.
        # Let's adjust fetch_initial_data to set self.data to resampled_ohlcv_hist_data
        # (Modify fetch_initial_data to store raw OHLCV in self.data)

        # For now, let's assume self.data IS the full history of OHLCV bars
        # And apply_live_indicators runs on it to get features.
        
        # This re-features the entire history + new bar each time - inefficient for very long self.data
        # Better: self.data is OHLCV. Take tail(N), apply features.
        # For now, let's work with current self.data structure (already has features from history)
        # This is tricky: how to add a new bar and its features?

        # --- Simplification for this iteration: ---
        # Assume self.data is the historical dataset with features.
        # We form ONE new bar from stream, calculate its features based on self.data, predict.
        # This means `apply_live_indicators` needs to work on `self.data` + new bar info.
        
        latest_bar_ohlcv = newly_formed_bars_live.iloc[-1:] # The newest complete bar from stream
        
        # Combine with enough history from self.data to calculate all indicators for this new bar
        # self.data should have OHLC columns if indicators need them.
        # Let's assume self.data from fetch_initial_data has 'open', 'high', 'low', 'price', 'volume'
        
        if self.data is None or self.data.empty: # Should be caught by on_success init
            print("self.data is empty in resample_and_trade"); return

        # Take enough lookback from self.data for indicators
        # Max lookback needed by any indicator (e.g. 50 for SMA50, + lags for returns)
        lookback_window_size = 100 + self.lags # Generous window
        combined_df_for_features = pd.concat([self.data.tail(lookback_window_size), latest_bar_ohlcv])
        
        # Apply indicators to this combined DataFrame
        df_with_new_features, live_feature_names = self.apply_live_indicators(combined_df_for_features)

        if df_with_new_features is None or df_with_new_features.empty or not live_feature_names:
            # print(f"Could not generate features for new bar {latest_bar_ohlcv.index[-1]}")
            return
            
        # The features for the latest bar are in the last row of df_with_new_features
        self.feature_names_for_model = live_feature_names # Update if they changed
        latest_raw_features = df_with_new_features[self.feature_names_for_model].iloc[-1:]

        if latest_raw_features.isnull().any().any():
            print(f"Raw features for new bar {latest_bar_ohlcv.index[-1]} contain NaNs. Skipping."); return

        # Scale these raw features
        if self.scaler is None:
            print("Scaler not loaded. Cannot scale features for prediction."); return
        
        try:
            scaled_features_for_prediction = self.scaler.transform(latest_raw_features)
        except ValueError as ve:
            print(f"Error scaling features: {ve}. Shapes: raw={latest_raw_features.shape}")
            print(f"Scaler n_features_in_: {self.scaler.n_features_in_ if hasattr(self.scaler, 'n_features_in_') else 'N/A'}")
            print(f"Feature names model expects (from training): {self.scaler.feature_names_in_ if hasattr(self.scaler, 'feature_names_in_') else 'N/A'}")
            print(f"Feature names generated live: {self.feature_names_for_model}")
            print(f"Latest raw features:\n{latest_raw_features}")
            return


        if self.model is None:
            print("Model not loaded. Cannot predict."); return
        
        # Update self.data and self.last_bar_time with the new bar that has features
        # This ensures self.data grows with bars that have full features for next iteration's lookback
        new_bar_with_features_to_append = df_with_new_features.iloc[-1:]
        self.data = pd.concat([self.data, new_bar_with_features_to_append])
        self.data = self.data[~self.data.index.duplicated(keep='last')] # Remove duplicates if any overlap
        self.last_bar_time = latest_bar_ohlcv.index[-1] # Update to the timestamp of the bar we just processed


        try:
            prediction = self.model.predict(scaled_features_for_prediction)[0]
            print(f"\nPrediction for {self.instrument} (bar ending {self.last_bar_time}, signal for next bar): {'LONG' if prediction == 1 else 'SHORT' if prediction == -1 else 'NEUTRAL'}")
            self.execute_trade_logic(prediction)
        except Exception as e:
            print(f"Error during prediction or trade execution: {e}"); import traceback; traceback.print_exc()


    def execute_trade_logic(self, prediction):
        print(f"TradeLogic | Current Pos: {self.position}, Signal: {'L' if prediction == 1 else 'S' if prediction == -1 else 'N'}, Units: {self.units}")

        if prediction == 1: # Signal to Go LONG
            if self.position == 0: # If neutral, open long
                print(f"Action: Go LONG {self.units} {self.instrument}")
                self.create_order(self.instrument, units=self.units, suppress=True, ret=True)
                self.position = 1
            elif self.position == -1: # If short, close short and open long (reverse)
                print(f"Action: Close SHORT, Go LONG {self.units} {self.instrument}")
                # Order to close short position (amount is 2 * self.units if short units are -self.units)
                # Or simply, an order of self.units closes a short of -self.units and opens a long of 0.
                # Then another self.units order.
                # Simpler: OANDA handles net positions. If short 10k, order for 20k long = net 10k long.
                self.create_order(self.instrument, units=2 * self.units, suppress=True, ret=True)
                self.position = 1
            # else: print("Already LONG or no change in signal. Holding.")
        elif prediction == -1: # Signal to Go SHORT
            if self.position == 0: # If neutral, open short
                print(f"Action: Go SHORT {-self.units} {self.instrument}")
                self.create_order(self.instrument, units=-self.units, suppress=True, ret=True)
                self.position = -1
            elif self.position == 1: # If long, close long and open short (reverse)
                print(f"Action: Close LONG, Go SHORT {-self.units} {self.instrument}")
                self.create_order(self.instrument, units=-2 * self.units, suppress=True, ret=True)
                self.position = -1
            # else: print("Already SHORT or no change in signal. Holding.")
        elif prediction == 0: # Signal to Go NEUTRAL (if your model predicts this)
            if self.position == 1: # If long, close it
                print(f"Action: Close LONG position")
                self.create_order(self.instrument, units=-self.units, suppress=True, ret=True)
                self.position = 0
            elif self.position == -1: # If short, close it
                print(f"Action: Close SHORT position")
                self.create_order(self.instrument, units=self.units, suppress=True, ret=True)
                self.position = 0
            # else: print("Already NEUTRAL. Holding.")
        
        self.report_balance()

    def report_balance(self):
        # ... (report_balance method remains the same as your previous working version) ...
        try:
            summary = self.get_account_summary()
            balance = summary.get('balance', 'N/A')
            pl = summary.get('pl', 'N/A') 
            unrealized_pl = summary.get('unrealizedPL', 'N/A')
            
            oanda_pos_qty = 0
            oanda_pos_side = "NEUTRAL"
            positions = self.get_positions() 

            for pos_data in positions:
                if pos_data['instrument'] == self.instrument:
                    long_units_str = pos_data.get('long', {}).get('units', '0') 
                    short_units_str = pos_data.get('short', {}).get('units', '0')
                    try: long_units_val = int(float(long_units_str))
                    except ValueError: long_units_val = 0
                    try: short_units_val = int(float(short_units_str))
                    except ValueError: short_units_val = 0

                    if long_units_val != 0:
                        oanda_pos_qty = long_units_val
                        oanda_pos_side = "LONG"
                    elif short_units_val != 0:
                        oanda_pos_qty = -short_units_val 
                        oanda_pos_side = "SHORT"
                    break 
            
            print(f"Balance: {balance} | P&L: {pl} | Unrealized P&L: {unrealized_pl} | "
                  f"OANDA {self.instrument} Pos: {oanda_pos_side} {oanda_pos_qty} | Internal Pos Tracker: {self.position}")
        except Exception as e:
            print(f"Could not retrieve/parse account balance: {type(e).__name__} - {e}")


# --- Example Usage ---
if __name__ == "__main__":
    conf_file = "oanda.cfg"
    instrument = "EUR_USD"
    bar_length_live = "1min"
    lags_live = 5 # MUST MATCH THE LAGS THE MODEL WAS TRAINED WITH (used by LaggedReturnsFeature)
    units_to_trade = 10000 
    
    # --- Define the list of indicators to apply LIVE ---
    # THIS LIST MUST MATCH EXACTLY (types and parameters) THOSE USED IN ModelTrainer.py
    # TO PRODUCE THE SAME RAW FEATURES BEFORE SCALING.
    live_indicators = [
        LaggedReturnsFeature(num_lags=lags_live, price_col='price'),
        MovingAverageFeature(window=10, ma_type='ema', price_col='price'),
        MovingAverageFeature(window=20, ma_type='ema', price_col='price'),
        MovingAverageFeature(window=50, ma_type='sma', price_col='price'),
        MACrossover(short_window=10, long_window=20, ma_type='ema', price_col='price'),
        RSIFeature(window=14, price_col='price'),
        MACDFeature(fast_period=12, slow_period=26, signal_period=9, price_col='price'),
        ATRFeature(window=14, high_col='high', low_col='low', close_col='price'),
        BollingerBandsFeatures(window=20, num_std_dev=2, price_col='price'),
        VolatilitySTDFeature(window=20, price_col='price'), # Will use 'price' to make 'returns'
        HourFeature(cyclical_transform=True),
        DayOfWeekFeature(cyclical_transform=True)
    ]
    
    # IMPORTANT: Construct the model and scaler filenames
    chosen_model_type_from_training = "LightGBM" # Example: Change this to the actual best model type
    # This pattern should match ModelTrainer.py's save_filename_final_model & save_filename_final_scaler
    base_filename = f"ml_trader_model_{chosen_model_type_from_training}_Scaled_VecSharpe_{instrument.replace('_','')}_{bar_length_live}_{lags_live}lags_trained"
    model_file_to_load = f"{base_filename}.pkl"
    scaler_file_to_load = f"{base_filename}_scaler.pkl" # As saved by ModelTrainer.py
    
    print(f"Attempting to load model: {model_file_to_load}")
    print(f"Attempting to load scaler: {scaler_file_to_load}")

    trader = MLTrader(conf_file=conf_file,
                      instrument=instrument,
                      bar_length=bar_length_live,
                      lags=lags_live, # Base lags for LaggedReturnsFeature
                      units=units_to_trade,
                      model_filename=model_file_to_load,
                      scaler_filename=scaler_file_to_load, # Pass scaler filename
                      live_indicators_to_apply=live_indicators, # Pass indicator instances
                      fallback_model_template=None)

    if not trader.fetch_initial_data(days_for_features=10, granularity="S5"): # Fetch more days for longer indicators
         print("Failed to init trader with hist data. Live predictions might be delayed/inaccurate.")
         # exit() # Optional: exit if hist data is crucial

    if trader.model is None or trader.scaler is None:
        print("CRITICAL: Model or Scaler not loaded. Trading cannot proceed. Exiting.")
        exit()

    print("\n--- Starting Live Stream ---")
    try:
        trader.stream_data(trader.instrument, stop=500) # Test with 500 ticks
    except KeyboardInterrupt:
        print("\nStreaming stopped by user.")
    except Exception as e:
        print(f"\nAn error occurred during streaming: {e}"); import traceback; traceback.print_exc()
    finally:
        # ... (finalizing and closing logic as before) ...
        print("\n--- Finalizing ---")
        if trader.position == 1:
            print(f"Final Close: Closing LONG position of {trader.units} {trader.instrument}.")
            trader.create_order(trader.instrument, units=-trader.units, suppress=True, ret=True)
        elif trader.position == -1:
            print(f"Final Close: Closing SHORT position of {trader.units} {trader.instrument}.")
            trader.create_order(trader.instrument, units=trader.units, suppress=True, ret=True)
        trader.position = 0
        trader.report_balance()
        print("Trading session ended.")